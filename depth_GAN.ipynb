{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "depth-GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADI201998/sparse-to-depth-GAN/blob/master/depth_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPgZvLDqiEBg",
        "colab_type": "code",
        "outputId": "26348c5d-5364-4436-8b41-ce9d57d9c4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEiDNuKyMLSR",
        "colab_type": "code",
        "outputId": "285ec5aa-6acd-4a3f-d16e-0972638bd0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 96kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.5)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY77QxKKjfIQ",
        "colab_type": "code",
        "outputId": "a3ef5ce0-d1f4-4921-9f86-85c1e404ee84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import ZeroPadding2D, Activation, BatchNormalization, MaxPooling2D, Dense, Add, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Input, Dropout\n",
        "from keras import losses\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "input_shape_generator = (480, 640, 4)\n",
        "input_shape_discriminator = (480, 640, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zyhBN_naEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_to_sparse(rgb, depth):\n",
        "        \"\"\"\n",
        "        Samples pixels with `num_samples`/#pixels probability in `depth`.\n",
        "        Only pixels with a maximum depth of `max_depth` are considered.\n",
        "        If no `max_depth` is given, samples in all pixels\n",
        "        \"\"\"\n",
        "        # max depth is np.inf\n",
        "        mask_keep = depth > 0\n",
        "        if np.inf is not np.inf:\n",
        "                mask_keep = np.bitwise_and(mask_keep, depth <= 1.0)\n",
        "        n_keep = np.count_nonzero(mask_keep)\n",
        "        if n_keep == 0:\n",
        "                return mask_keep\n",
        "        else:\n",
        "                prob = float(200) / n_keep\n",
        "                return np.bitwise_and(mask_keep, np.random.uniform(0, 1, depth.shape) < prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tas7FKuroGay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sparse_depth(rgb, depth):\n",
        "        mask_keep = dense_to_sparse(rgb, depth)\n",
        "        sparse_depth = np.zeros(depth.shape)\n",
        "        sparse_depth[mask_keep] = depth[mask_keep]\n",
        "        return sparse_depth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgeJxqfeoJM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_rgbd(rgb, depth):\n",
        "        sparse_depth = create_sparse_depth(rgb, depth)\n",
        "        rgbd = np.append(rgb, np.expand_dims(sparse_depth, axis=2), axis=2)\n",
        "        rgbd = rgbd[np.newaxis, :, :, :]\n",
        "        return rgbd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6248QBOoSJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset():\n",
        "        X_train = np.empty((1400, 480, 640, 4))\n",
        "        Y_train = np.empty((1400, 480, 640, 1))\n",
        "        X_test = np.empty((280, 480, 640, 4))\n",
        "        Y_test = np.empty((280, 480, 640, 1))\n",
        "        rootdir_train = '/content/drive/My Drive/Dataset/train/'\n",
        "        rootdir_val = '/content/drive/My Drive/Dataset/val/official/'\n",
        "        print(\"TRAIN\")\n",
        "        j = i = 0\n",
        "        for subdir, dirs, files in sorted(os.walk(rootdir_train)):\n",
        "                print(j)\n",
        "                files = sorted(files)\n",
        "                print(len(files))\n",
        "                if len(files) != 0:\n",
        "                        for k in range(5):\n",
        "                              #print(i)\n",
        "                              path = subdir + '/' + files[k]\n",
        "                              h5f = h5py.File(path, \"r\")\n",
        "                              rgb = np.array(h5f['rgb'])\n",
        "                              rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                              depth = np.array(h5f['depth'])\n",
        "                              depth  = depth /9\n",
        "                              #cv2.imshow(\"rgb\", rgb)\n",
        "                              #cv2.imshow(\"depth\", depth)\n",
        "                              X_train[i] = create_rgbd(rgb, depth)\n",
        "                              Y_train[i] = depth[:, :, np.newaxis]\n",
        "                              i = i+1\n",
        "                              #cv2.waitKey(0)\n",
        "                              #cv2.destroyAllWindows()\n",
        "                        j = j+1\n",
        "                                                \n",
        "        print(\"TEST\")\n",
        "        for subdir, dirs, files in sorted(os.walk(rootdir_val)):\n",
        "                files = sorted(files)\n",
        "                print(len(files))\n",
        "                for i in range(280):\n",
        "                        print(i)\n",
        "                        path = subdir + '/' + files[i]\n",
        "                        h5f = h5py.File(path, \"r\")\n",
        "                        rgb = np.array(h5f['rgb'])\n",
        "                        rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                        depth = np.array(h5f['depth'])\n",
        "                        depth  = depth /9\n",
        "                        X_test[i] = create_rgbd(rgb, depth)\n",
        "                        Y_test[i] = depth[:, :, np.newaxis]\n",
        "        print(\"OVER\")\n",
        "        print(\"Done\")\n",
        "        return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWA7W5hogfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model():\n",
        "    #encoder\n",
        "    X_input = Input(input_shape_generator)\n",
        "    X = Conv2D(64, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X_input)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 3,activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 1, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X1 = X\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, 1, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X2 = X\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X3 = X \n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((1,2))(X)\n",
        "\n",
        "    #decoder\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((1,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Add()([X,X3])\n",
        "    X = Dropout(0.5)(X)\n",
        "\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Add()([X,X2])\n",
        "\n",
        "    X = Conv2D(256, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Add()([X,X1])\n",
        "    X = Dropout(0.5)(X)\n",
        "\n",
        "    X = Conv2D(128, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Conv2D(64, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Dropout(0.5)(X)\n",
        "    X = Conv2D(1, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X_out = BatchNormalization(axis=3)(X)\n",
        "\n",
        "    model = Model(input = X_input, output = X_out, name='Generator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5RD2_qIo3tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def discriminator_model():\n",
        "    X_input = Input(input_shape_discriminator)\n",
        "    X = Conv2D(64, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X_input)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(1024, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Flatten()(X)\n",
        "    X_out = Dense(1, activation = 'sigmoid')(X)\n",
        "\n",
        "    model = Model(input =  X_input, output = X_out, name='Discriminator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK6HeX48s0K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_model(generator, discriminator):\n",
        "    gan_input = Input(input_shape_generator)\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    gan = Model(input = gan_input, output = [x, gan_output], name='GAN')\n",
        "    return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spUL150-tTLm",
        "colab_type": "code",
        "outputId": "b6f049d1-0a39-42ce-a245-37593633f6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 10\n",
        "epochs = 140\n",
        "input_shape = (480, 640, 4)\n",
        "# 280 x 20 \n",
        "##  INPUT  ##\n",
        "print(\"----------------------GETTING INPUTS-----------------------\")\n",
        "X_train, Y_train, X_test, Y_test = dataset()\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
        "\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "## INITIALIZE MODELS  ##\n",
        "print(\"----------------------INITIALIZING MODELS-----------------------\")\n",
        "generator = generator_model()       \n",
        "discriminator = discriminator_model()\n",
        "gan = gan_model(generator, discriminator)\n",
        "\n",
        "## COMPILE MODELS ##\n",
        "print(\"----------------------COMPILING MODELS-----------------------\")\n",
        "discriminator.trainable = True\n",
        "discriminator.compile(optimizer = Adam(4e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "discriminator.trainable = False\n",
        "gan.compile(optimizer = Adam(4e-4), loss = ['mae', 'binary_crossentropy'], loss_weights=[1, 0.001])\n",
        "discriminator.trainable = True\n",
        "\n",
        "print(\"----------------------STARTING TRAINING-----------------------\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # SELECTING RANDOM BATCH OF IMAGES                      \n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    op_imgs = Y_train[idx]\n",
        "\n",
        "    # GENERATE NEW IMAGES\n",
        "    generated_img = generator.predict(imgs)\n",
        "\n",
        "    # TRAIN DISCRIMINATOR\n",
        "    d_loss_real = discriminator.train_on_batch(op_imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_img, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    discriminator.trainable = False\n",
        "    # TRAIN GENERATOR\n",
        "    g_loss = gan.train_on_batch(imgs, [op_imgs, valid])                           #fill \n",
        "    \n",
        "    print(\"Epochs : \",epoch,\" D_loss : \", d_loss, \" G_loss : \", g_loss)\n",
        "    discriminator.trainable = True\n",
        "\n",
        "print(\"----------------------TRAINING ENDS-----------------------\")\n",
        "generator.save('generator_model.h5')\n",
        "discriminator.save('disriminator_model.h5')\n",
        "predicted_imgs = generator.predict(X_test)\n",
        "for i in range(20):\n",
        "    #rgb = np.squeeze(X_test[i], axis=0)\n",
        "    rgb = X_test[i, :, :, 0:3]\n",
        "    print(rgb.shape)\n",
        "    depth = Y_test[i, :, :, :]\n",
        "    print(depth.shape)\n",
        "    depth_pred = predicted_imgs[i, :, :, :]\n",
        "    print(depth_pred.shape)\n",
        "    plt.imshow(rgb)\n",
        "    plt.show()\n",
        "    #cv2_imshow(str(i)+\"rgb\", rgb)\n",
        "    #depth = np.squeeze(Y_test[i], axis=0)\n",
        "    \n",
        "    #cv2_imshow(str(i)+\"depth\", depth)\n",
        "    plt.imshow(depth)\n",
        "    plt.show()\n",
        "    #depth_pred = np.squeeze(generated_img[i], axis=0)\n",
        "    #cv2_imshow(str(i)+\"depth_pred\", depth_pred)\n",
        "    plt.imshow(depth_pred)\n",
        "    plt.show()\n",
        "    #cv2.waitKey(0)\n",
        "    #cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------GETTING INPUTS-----------------------\n",
            "TRAIN\n",
            "0\n",
            "0\n",
            "0\n",
            "20\n",
            "1\n",
            "20\n",
            "2\n",
            "20\n",
            "3\n",
            "20\n",
            "4\n",
            "20\n",
            "5\n",
            "20\n",
            "6\n",
            "20\n",
            "7\n",
            "20\n",
            "8\n",
            "20\n",
            "9\n",
            "20\n",
            "10\n",
            "20\n",
            "11\n",
            "20\n",
            "12\n",
            "20\n",
            "13\n",
            "20\n",
            "14\n",
            "20\n",
            "15\n",
            "20\n",
            "16\n",
            "20\n",
            "17\n",
            "20\n",
            "18\n",
            "20\n",
            "19\n",
            "20\n",
            "20\n",
            "20\n",
            "21\n",
            "20\n",
            "22\n",
            "20\n",
            "23\n",
            "20\n",
            "24\n",
            "20\n",
            "25\n",
            "20\n",
            "26\n",
            "20\n",
            "27\n",
            "20\n",
            "28\n",
            "20\n",
            "29\n",
            "20\n",
            "30\n",
            "20\n",
            "31\n",
            "20\n",
            "32\n",
            "20\n",
            "33\n",
            "20\n",
            "34\n",
            "20\n",
            "35\n",
            "20\n",
            "36\n",
            "20\n",
            "37\n",
            "20\n",
            "38\n",
            "20\n",
            "39\n",
            "20\n",
            "40\n",
            "20\n",
            "41\n",
            "20\n",
            "42\n",
            "20\n",
            "43\n",
            "20\n",
            "44\n",
            "20\n",
            "45\n",
            "20\n",
            "46\n",
            "20\n",
            "47\n",
            "20\n",
            "48\n",
            "20\n",
            "49\n",
            "20\n",
            "50\n",
            "20\n",
            "51\n",
            "20\n",
            "52\n",
            "20\n",
            "53\n",
            "11\n",
            "54\n",
            "20\n",
            "55\n",
            "20\n",
            "56\n",
            "20\n",
            "57\n",
            "20\n",
            "58\n",
            "20\n",
            "59\n",
            "20\n",
            "60\n",
            "20\n",
            "61\n",
            "20\n",
            "62\n",
            "20\n",
            "63\n",
            "20\n",
            "64\n",
            "20\n",
            "65\n",
            "20\n",
            "66\n",
            "20\n",
            "67\n",
            "20\n",
            "68\n",
            "20\n",
            "69\n",
            "20\n",
            "70\n",
            "20\n",
            "71\n",
            "20\n",
            "72\n",
            "20\n",
            "73\n",
            "20\n",
            "74\n",
            "20\n",
            "75\n",
            "20\n",
            "76\n",
            "20\n",
            "77\n",
            "20\n",
            "78\n",
            "20\n",
            "79\n",
            "20\n",
            "80\n",
            "20\n",
            "81\n",
            "20\n",
            "82\n",
            "20\n",
            "83\n",
            "20\n",
            "84\n",
            "20\n",
            "85\n",
            "20\n",
            "86\n",
            "20\n",
            "87\n",
            "20\n",
            "88\n",
            "20\n",
            "89\n",
            "20\n",
            "90\n",
            "20\n",
            "91\n",
            "20\n",
            "92\n",
            "20\n",
            "93\n",
            "20\n",
            "94\n",
            "20\n",
            "95\n",
            "20\n",
            "96\n",
            "20\n",
            "97\n",
            "20\n",
            "98\n",
            "20\n",
            "99\n",
            "20\n",
            "100\n",
            "20\n",
            "101\n",
            "20\n",
            "102\n",
            "20\n",
            "103\n",
            "20\n",
            "104\n",
            "20\n",
            "105\n",
            "20\n",
            "106\n",
            "20\n",
            "107\n",
            "20\n",
            "108\n",
            "20\n",
            "109\n",
            "20\n",
            "110\n",
            "20\n",
            "111\n",
            "20\n",
            "112\n",
            "20\n",
            "113\n",
            "20\n",
            "114\n",
            "20\n",
            "115\n",
            "20\n",
            "116\n",
            "20\n",
            "117\n",
            "20\n",
            "118\n",
            "20\n",
            "119\n",
            "20\n",
            "120\n",
            "20\n",
            "121\n",
            "20\n",
            "122\n",
            "20\n",
            "123\n",
            "20\n",
            "124\n",
            "20\n",
            "125\n",
            "20\n",
            "126\n",
            "20\n",
            "127\n",
            "20\n",
            "128\n",
            "20\n",
            "129\n",
            "20\n",
            "130\n",
            "20\n",
            "131\n",
            "20\n",
            "132\n",
            "20\n",
            "133\n",
            "20\n",
            "134\n",
            "20\n",
            "135\n",
            "20\n",
            "136\n",
            "20\n",
            "137\n",
            "20\n",
            "138\n",
            "20\n",
            "139\n",
            "20\n",
            "140\n",
            "20\n",
            "141\n",
            "20\n",
            "142\n",
            "20\n",
            "143\n",
            "20\n",
            "144\n",
            "22\n",
            "145\n",
            "20\n",
            "146\n",
            "20\n",
            "147\n",
            "20\n",
            "148\n",
            "20\n",
            "149\n",
            "20\n",
            "150\n",
            "20\n",
            "151\n",
            "20\n",
            "152\n",
            "20\n",
            "153\n",
            "20\n",
            "154\n",
            "20\n",
            "155\n",
            "20\n",
            "156\n",
            "20\n",
            "157\n",
            "20\n",
            "158\n",
            "20\n",
            "159\n",
            "20\n",
            "160\n",
            "20\n",
            "161\n",
            "20\n",
            "162\n",
            "20\n",
            "163\n",
            "20\n",
            "164\n",
            "20\n",
            "165\n",
            "20\n",
            "166\n",
            "20\n",
            "167\n",
            "20\n",
            "168\n",
            "20\n",
            "169\n",
            "20\n",
            "170\n",
            "20\n",
            "171\n",
            "20\n",
            "172\n",
            "20\n",
            "173\n",
            "20\n",
            "174\n",
            "20\n",
            "175\n",
            "20\n",
            "176\n",
            "20\n",
            "177\n",
            "20\n",
            "178\n",
            "20\n",
            "179\n",
            "20\n",
            "180\n",
            "20\n",
            "181\n",
            "20\n",
            "182\n",
            "20\n",
            "183\n",
            "20\n",
            "184\n",
            "20\n",
            "185\n",
            "20\n",
            "186\n",
            "20\n",
            "187\n",
            "20\n",
            "188\n",
            "20\n",
            "189\n",
            "20\n",
            "190\n",
            "20\n",
            "191\n",
            "20\n",
            "192\n",
            "20\n",
            "193\n",
            "20\n",
            "194\n",
            "20\n",
            "195\n",
            "20\n",
            "196\n",
            "20\n",
            "197\n",
            "20\n",
            "198\n",
            "20\n",
            "199\n",
            "20\n",
            "200\n",
            "20\n",
            "201\n",
            "20\n",
            "202\n",
            "20\n",
            "203\n",
            "20\n",
            "204\n",
            "20\n",
            "205\n",
            "20\n",
            "206\n",
            "20\n",
            "207\n",
            "20\n",
            "208\n",
            "20\n",
            "209\n",
            "20\n",
            "210\n",
            "20\n",
            "211\n",
            "20\n",
            "212\n",
            "20\n",
            "213\n",
            "20\n",
            "214\n",
            "20\n",
            "215\n",
            "20\n",
            "216\n",
            "20\n",
            "217\n",
            "20\n",
            "218\n",
            "20\n",
            "219\n",
            "20\n",
            "220\n",
            "20\n",
            "221\n",
            "20\n",
            "222\n",
            "20\n",
            "223\n",
            "20\n",
            "224\n",
            "20\n",
            "225\n",
            "20\n",
            "226\n",
            "20\n",
            "227\n",
            "20\n",
            "228\n",
            "20\n",
            "229\n",
            "20\n",
            "230\n",
            "20\n",
            "231\n",
            "20\n",
            "232\n",
            "20\n",
            "233\n",
            "20\n",
            "234\n",
            "20\n",
            "235\n",
            "20\n",
            "236\n",
            "20\n",
            "237\n",
            "20\n",
            "238\n",
            "20\n",
            "239\n",
            "20\n",
            "240\n",
            "20\n",
            "241\n",
            "20\n",
            "242\n",
            "20\n",
            "243\n",
            "20\n",
            "244\n",
            "20\n",
            "245\n",
            "20\n",
            "246\n",
            "20\n",
            "247\n",
            "20\n",
            "248\n",
            "20\n",
            "249\n",
            "20\n",
            "250\n",
            "20\n",
            "251\n",
            "20\n",
            "252\n",
            "20\n",
            "253\n",
            "20\n",
            "254\n",
            "20\n",
            "255\n",
            "20\n",
            "256\n",
            "20\n",
            "257\n",
            "20\n",
            "258\n",
            "20\n",
            "259\n",
            "20\n",
            "260\n",
            "20\n",
            "261\n",
            "20\n",
            "262\n",
            "20\n",
            "263\n",
            "20\n",
            "264\n",
            "20\n",
            "265\n",
            "20\n",
            "266\n",
            "20\n",
            "267\n",
            "20\n",
            "268\n",
            "20\n",
            "269\n",
            "20\n",
            "270\n",
            "20\n",
            "271\n",
            "20\n",
            "272\n",
            "20\n",
            "273\n",
            "20\n",
            "274\n",
            "20\n",
            "275\n",
            "20\n",
            "276\n",
            "20\n",
            "277\n",
            "20\n",
            "278\n",
            "20\n",
            "279\n",
            "20\n",
            "TEST\n",
            "654\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 16:19:31.705237 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0815 16:19:31.771388 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0815 16:19:31.783904 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0815 16:19:31.809412 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0815 16:19:31.810466 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "279\n",
            "OVER\n",
            "Done\n",
            "(1400, 480, 640, 4) (1400, 480, 640, 1) (280, 480, 640, 4) (280, 480, 640, 1)\n",
            "----------------------INITIALIZING MODELS-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0815 16:19:34.949231 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0815 16:19:35.025262 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0815 16:19:35.704842 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0815 16:19:35.794645 140622130927488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Generator\", inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Discriminator\", inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"GAN\", inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
            "  \n",
            "W0815 16:19:38.151221 140622130927488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0815 16:19:38.159835 140622130927488 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------COMPILING MODELS-----------------------\n",
            "----------------------STARTING TRAINING-----------------------\n",
            "Epochs :  0  D_loss :  [8.3680105 0.3      ]  G_loss :  [797.48804, 0.7982863, 1.1920933e-07]\n",
            "Epochs :  1  D_loss :  [7.9711924 0.5      ]  G_loss :  [708.6973, 0.70940673, 1.1920933e-07]\n",
            "Epochs :  2  D_loss :  [7.9711924 0.5      ]  G_loss :  [596.3638, 0.5969607, 1.1920933e-07]\n",
            "Epochs :  3  D_loss :  [7.9711924 0.5      ]  G_loss :  [542.35175, 0.54289466, 1.1920933e-07]\n",
            "Epochs :  4  D_loss :  [7.9711924 0.5      ]  G_loss :  [466.17194, 0.46663857, 1.1920933e-07]\n",
            "Epochs :  5  D_loss :  [7.9711924 0.5      ]  G_loss :  [415.04416, 0.41545963, 1.1920933e-07]\n",
            "Epochs :  6  D_loss :  [7.9711924 0.5      ]  G_loss :  [414.0516, 0.41446608, 1.1920933e-07]\n",
            "Epochs :  7  D_loss :  [7.9711924 0.5      ]  G_loss :  [422.00174, 0.42242417, 1.1920933e-07]\n",
            "Epochs :  8  D_loss :  [7.9711924 0.5      ]  G_loss :  [403.77985, 0.40418404, 1.1920933e-07]\n",
            "Epochs :  9  D_loss :  [7.9711924 0.5      ]  G_loss :  [411.43878, 0.41185063, 1.1920933e-07]\n",
            "Epochs :  10  D_loss :  [7.9711924 0.5      ]  G_loss :  [356.5478, 0.3569047, 1.1920933e-07]\n",
            "Epochs :  11  D_loss :  [7.9711924 0.5      ]  G_loss :  [341.10715, 0.3414486, 1.1920933e-07]\n",
            "Epochs :  12  D_loss :  [7.9711924 0.5      ]  G_loss :  [363.9749, 0.36433926, 1.1920933e-07]\n",
            "Epochs :  13  D_loss :  [7.9711924 0.5      ]  G_loss :  [329.73328, 0.33006334, 1.1920933e-07]\n",
            "Epochs :  14  D_loss :  [7.9711924 0.5      ]  G_loss :  [366.99377, 0.36736113, 1.1920933e-07]\n",
            "Epochs :  15  D_loss :  [7.9711924 0.5      ]  G_loss :  [322.12357, 0.32244602, 1.1920933e-07]\n",
            "Epochs :  16  D_loss :  [7.9711924 0.5      ]  G_loss :  [302.60962, 0.30291253, 1.1920933e-07]\n",
            "Epochs :  17  D_loss :  [7.9711924 0.5      ]  G_loss :  [349.6431, 0.34999308, 1.1920933e-07]\n",
            "Epochs :  18  D_loss :  [7.9711924 0.5      ]  G_loss :  [315.39453, 0.31571025, 1.1920933e-07]\n",
            "Epochs :  19  D_loss :  [7.9711924 0.5      ]  G_loss :  [302.28076, 0.30258334, 1.1920933e-07]\n",
            "Epochs :  20  D_loss :  [7.9711924 0.5      ]  G_loss :  [323.55475, 0.32387862, 1.1920933e-07]\n",
            "Epochs :  21  D_loss :  [7.9711924 0.5      ]  G_loss :  [359.86203, 0.36022225, 1.1920933e-07]\n",
            "Epochs :  22  D_loss :  [7.9711924 0.5      ]  G_loss :  [332.07407, 0.33240646, 1.1920933e-07]\n",
            "Epochs :  23  D_loss :  [7.9711924 0.5      ]  G_loss :  [290.52948, 0.2908203, 1.1920933e-07]\n",
            "Epochs :  24  D_loss :  [7.9711924 0.5      ]  G_loss :  [302.55914, 0.30286202, 1.1920933e-07]\n",
            "Epochs :  25  D_loss :  [7.9711924 0.5      ]  G_loss :  [349.08823, 0.34943765, 1.1920933e-07]\n",
            "Epochs :  26  D_loss :  [7.9711924 0.5      ]  G_loss :  [318.2382, 0.31855676, 1.1920933e-07]\n",
            "Epochs :  27  D_loss :  [7.9711924 0.5      ]  G_loss :  [336.36136, 0.33669806, 1.1920933e-07]\n",
            "Epochs :  28  D_loss :  [7.9711924 0.5      ]  G_loss :  [392.45184, 0.39284468, 1.1920933e-07]\n",
            "Epochs :  29  D_loss :  [7.9711924 0.5      ]  G_loss :  [310.79224, 0.31110334, 1.1920933e-07]\n",
            "Epochs :  30  D_loss :  [7.9711924 0.5      ]  G_loss :  [310.19638, 0.31050688, 1.1920933e-07]\n",
            "Epochs :  31  D_loss :  [7.9711924 0.5      ]  G_loss :  [270.50687, 0.27077764, 1.1920933e-07]\n",
            "Epochs :  32  D_loss :  [7.9711924 0.5      ]  G_loss :  [359.25122, 0.35961083, 1.1920933e-07]\n",
            "Epochs :  33  D_loss :  [7.9711924 0.5      ]  G_loss :  [348.37015, 0.34871885, 1.1920933e-07]\n",
            "Epochs :  34  D_loss :  [7.9711924 0.5      ]  G_loss :  [319.6758, 0.31999582, 1.1920933e-07]\n",
            "Epochs :  35  D_loss :  [7.9711924 0.5      ]  G_loss :  [372.99136, 0.37336472, 1.1920933e-07]\n",
            "Epochs :  36  D_loss :  [7.9711924 0.5      ]  G_loss :  [326.03278, 0.32635912, 1.1920933e-07]\n",
            "Epochs :  37  D_loss :  [7.9711924 0.5      ]  G_loss :  [309.18607, 0.30949557, 1.1920933e-07]\n",
            "Epochs :  38  D_loss :  [7.9711924 0.5      ]  G_loss :  [341.03152, 0.3413729, 1.1920933e-07]\n",
            "Epochs :  39  D_loss :  [7.9711924 0.5      ]  G_loss :  [287.9444, 0.28823262, 1.1920933e-07]\n",
            "Epochs :  40  D_loss :  [7.9711924 0.5      ]  G_loss :  [297.611, 0.2979089, 1.1920933e-07]\n",
            "Epochs :  41  D_loss :  [7.9711924 0.5      ]  G_loss :  [309.9033, 0.3102135, 1.1920933e-07]\n",
            "Epochs :  42  D_loss :  [7.9711924 0.5      ]  G_loss :  [311.67636, 0.31198835, 1.1920933e-07]\n",
            "Epochs :  43  D_loss :  [7.9711924 0.5      ]  G_loss :  [315.996, 0.3163123, 1.1920933e-07]\n",
            "Epochs :  44  D_loss :  [7.9711924 0.5      ]  G_loss :  [314.88394, 0.31519914, 1.1920933e-07]\n",
            "Epochs :  45  D_loss :  [7.9711924 0.5      ]  G_loss :  [282.75043, 0.28303346, 1.1920933e-07]\n",
            "Epochs :  46  D_loss :  [7.9711924 0.5      ]  G_loss :  [270.92285, 0.27119404, 1.1920933e-07]\n",
            "Epochs :  47  D_loss :  [7.9711924 0.5      ]  G_loss :  [351.32693, 0.3516786, 1.1920933e-07]\n",
            "Epochs :  48  D_loss :  [7.9711924 0.5      ]  G_loss :  [333.7987, 0.33413285, 1.1920933e-07]\n",
            "Epochs :  49  D_loss :  [7.9711924 0.5      ]  G_loss :  [296.74762, 0.29704466, 1.1920933e-07]\n",
            "Epochs :  50  D_loss :  [7.9711924 0.5      ]  G_loss :  [298.42242, 0.29872113, 1.1920933e-07]\n",
            "Epochs :  51  D_loss :  [7.9711924 0.5      ]  G_loss :  [330.2462, 0.33057675, 1.1920933e-07]\n",
            "Epochs :  52  D_loss :  [7.9711924 0.5      ]  G_loss :  [292.04175, 0.29233408, 1.1920933e-07]\n",
            "Epochs :  53  D_loss :  [7.9711924 0.5      ]  G_loss :  [360.35776, 0.3607185, 1.1920933e-07]\n",
            "Epochs :  54  D_loss :  [7.9711924 0.5      ]  G_loss :  [287.41034, 0.28769803, 1.1920933e-07]\n",
            "Epochs :  55  D_loss :  [7.9711924 0.5      ]  G_loss :  [272.98492, 0.27325818, 1.1920933e-07]\n",
            "Epochs :  56  D_loss :  [7.9711924 0.5      ]  G_loss :  [269.82178, 0.27009186, 1.1920933e-07]\n",
            "Epochs :  57  D_loss :  [7.9711924 0.5      ]  G_loss :  [320.69397, 0.32101497, 1.1920933e-07]\n",
            "Epochs :  58  D_loss :  [7.9711924 0.5      ]  G_loss :  [328.46094, 0.32878974, 1.1920933e-07]\n",
            "Epochs :  59  D_loss :  [7.9711924 0.5      ]  G_loss :  [314.68265, 0.31499764, 1.1920933e-07]\n",
            "Epochs :  60  D_loss :  [7.9711924 0.5      ]  G_loss :  [270.93393, 0.27120513, 1.1920933e-07]\n",
            "Epochs :  61  D_loss :  [7.9711924 0.5      ]  G_loss :  [341.76407, 0.34210616, 1.1920933e-07]\n",
            "Epochs :  62  D_loss :  [7.9711924 0.5      ]  G_loss :  [298.11746, 0.29841587, 1.1920933e-07]\n",
            "Epochs :  63  D_loss :  [7.9711924 0.5      ]  G_loss :  [296.67212, 0.2969691, 1.1920933e-07]\n",
            "Epochs :  64  D_loss :  [7.9711924 0.5      ]  G_loss :  [279.61816, 0.27989808, 1.1920933e-07]\n",
            "Epochs :  65  D_loss :  [7.9711924 0.5      ]  G_loss :  [286.41544, 0.28670213, 1.1920933e-07]\n",
            "Epochs :  66  D_loss :  [7.9711924 0.5      ]  G_loss :  [265.7991, 0.26606518, 1.1920933e-07]\n",
            "Epochs :  67  D_loss :  [7.9711924 0.5      ]  G_loss :  [280.44342, 0.28072414, 1.1920933e-07]\n",
            "Epochs :  68  D_loss :  [7.9711924 0.5      ]  G_loss :  [272.5149, 0.2727877, 1.1920933e-07]\n",
            "Epochs :  69  D_loss :  [7.9711924 0.5      ]  G_loss :  [256.41046, 0.25666714, 1.1920933e-07]\n",
            "Epochs :  70  D_loss :  [7.9711924 0.5      ]  G_loss :  [307.4578, 0.30776557, 1.1920933e-07]\n",
            "Epochs :  71  D_loss :  [7.9711924 0.5      ]  G_loss :  [260.56235, 0.26082316, 1.1920933e-07]\n",
            "Epochs :  72  D_loss :  [7.9711924 0.5      ]  G_loss :  [257.30878, 0.25756633, 1.1920933e-07]\n",
            "Epochs :  73  D_loss :  [7.9711924 0.5      ]  G_loss :  [277.4765, 0.27775425, 1.1920933e-07]\n",
            "Epochs :  74  D_loss :  [7.9711933 0.5      ]  G_loss :  [292.4408, 0.29273352, 1.1920933e-07]\n",
            "Epochs :  75  D_loss :  [7.9711924 0.5      ]  G_loss :  [289.6426, 0.28993255, 1.1920933e-07]\n",
            "Epochs :  76  D_loss :  [7.9711924 0.5      ]  G_loss :  [285.12625, 0.28541166, 1.1920933e-07]\n",
            "Epochs :  77  D_loss :  [7.9711924 0.5      ]  G_loss :  [253.27354, 0.25352708, 1.1920933e-07]\n",
            "Epochs :  78  D_loss :  [7.9711924 0.5      ]  G_loss :  [287.7192, 0.2880072, 2.0265585e-07]\n",
            "Epochs :  79  D_loss :  [7.9711924 0.5      ]  G_loss :  [263.9726, 0.26423684, 1.1920933e-07]\n",
            "Epochs :  80  D_loss :  [5.8266983 0.6      ]  G_loss :  [314.79623, 0.31511134, 1.1920933e-07]\n",
            "Epochs :  81  D_loss :  [6.3079247 0.6      ]  G_loss :  [255.86613, 0.2551242, 0.99704903]\n",
            "Epochs :  82  D_loss :  [4.7843947 0.7      ]  G_loss :  [309.36752, 0.3096772, 5.436086e-06]\n",
            "Epochs :  83  D_loss :  [6.8860593 0.5      ]  G_loss :  [288.37274, 0.2857495, 2.909008]\n",
            "Epochs :  84  D_loss :  [3.3651397 0.75     ]  G_loss :  [298.81915, 0.2896427, 9.466105]\n",
            "Epochs :  85  D_loss :  [3.188477 0.8     ]  G_loss :  [272.35913, 0.26917222, 3.4560974]\n",
            "Epochs :  86  D_loss :  [2.391358 0.85    ]  G_loss :  [320.4703, 0.3162895, 4.4970694]\n",
            "Epochs :  87  D_loss :  [0.7971193 0.95     ]  G_loss :  [261.3462, 0.25674334, 4.859599]\n",
            "Epochs :  88  D_loss :  [0.14486037 0.95      ]  G_loss :  [316.77094, 0.30256623, 14.507251]\n",
            "Epochs :  89  D_loss :  [1.5942386 0.9      ]  G_loss :  [276.11804, 0.26832733, 8.059048]\n",
            "Epochs :  90  D_loss :  [0.8014144 0.95     ]  G_loss :  [231.2803, 0.22183128, 9.670857]\n",
            "Epochs :  91  D_loss :  [2.1048527 0.85     ]  G_loss :  [278.39645, 0.2707091, 7.958061]\n",
            "Epochs :  92  D_loss :  [4.1027274e-06 1.0000000e+00]  G_loss :  [291.769, 0.28560597, 6.4486246]\n",
            "Epochs :  93  D_loss :  [0.62007135 0.95      ]  G_loss :  [271.57602, 0.25877103, 13.063751]\n",
            "Epochs :  94  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [295.86563, 0.2891248, 7.0299745]\n",
            "Epochs :  95  D_loss :  [0.17655486 0.95      ]  G_loss :  [257.824, 0.25091562, 7.1593003]\n",
            "Epochs :  96  D_loss :  [0.0436585 0.95     ]  G_loss :  [367.30115, 0.36444196, 3.2236278]\n",
            "Epochs :  97  D_loss :  [1.9827567 0.85     ]  G_loss :  [284.22452, 0.27139708, 13.098805]\n",
            "Epochs :  98  D_loss :  [0.02607409 1.        ]  G_loss :  [282.86594, 0.2702411, 12.895093]\n",
            "Epochs :  99  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [295.55792, 0.28133297, 14.506287]\n",
            "Epochs :  100  D_loss :  [2.2530423e-07 1.0000000e+00]  G_loss :  [285.0889, 0.27181533, 13.545374]\n",
            "Epochs :  101  D_loss :  [0.38281846 0.95      ]  G_loss :  [282.0474, 0.2686279, 13.688123]\n",
            "Epochs :  102  D_loss :  [3.7673865e-06 1.0000000e+00]  G_loss :  [254.5297, 0.24510393, 9.670869]\n",
            "Epochs :  103  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [339.7592, 0.33364558, 6.447238]\n",
            "Epochs :  104  D_loss :  [0.00273851 1.        ]  G_loss :  [260.0045, 0.25506455, 5.195009]\n",
            "Epochs :  105  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [273.6185, 0.26258928, 11.2918]\n",
            "Epochs :  106  D_loss :  [8.566272e-05 1.000000e+00]  G_loss :  [282.4234, 0.2723865, 10.30928]\n",
            "Epochs :  107  D_loss :  [2.0691147e-04 1.0000000e+00]  G_loss :  [267.59326, 0.25656718, 11.282667]\n",
            "Epochs :  108  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [281.06268, 0.26968774, 11.644615]\n",
            "Epochs :  109  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [302.01535, 0.29017085, 12.134666]\n",
            "Epochs :  110  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [245.51398, 0.23591253, 9.837355]\n",
            "Epochs :  111  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [358.38623, 0.34784853, 10.885556]\n",
            "Epochs :  112  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [281.781, 0.27257523, 9.478366]\n",
            "Epochs :  113  D_loss :  [0.7985839 0.95     ]  G_loss :  [257.5829, 0.24331753, 14.508682]\n",
            "Epochs :  114  D_loss :  [0.6067862 0.95     ]  G_loss :  [260.0093, 0.24413534, 16.118095]\n",
            "Epochs :  115  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [315.19904, 0.30989522, 5.6137238]\n",
            "Epochs :  116  D_loss :  [0.7971193 0.95     ]  G_loss :  [261.35327, 0.25314593, 8.460485]\n",
            "Epochs :  117  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [269.2904, 0.26182622, 7.7260284]\n",
            "Epochs :  118  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [272.70343, 0.26181, 11.155235]\n",
            "Epochs :  119  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [296.17545, 0.29324505, 3.223619]\n",
            "Epochs :  120  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [283.71045, 0.274491, 9.493935]\n",
            "Epochs :  121  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [267.23822, 0.25459835, 12.894476]\n",
            "Epochs :  122  D_loss :  [5.2770647e-06 1.0000000e+00]  G_loss :  [303.19583, 0.29009825, 13.387665]\n",
            "Epochs :  123  D_loss :  [1.5943178 0.9      ]  G_loss :  [253.53435, 0.24249378, 11.283069]\n",
            "Epochs :  124  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [312.59937, 0.301021, 11.879353]\n",
            "Epochs :  125  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [273.86353, 0.26606306, 8.066525]\n",
            "Epochs :  126  D_loss :  [7.6025987e-07 1.0000000e+00]  G_loss :  [270.41367, 0.26199952, 8.676145]\n",
            "Epochs :  127  D_loss :  [0.7971193 0.95     ]  G_loss :  [274.888, 0.26386875, 11.2831135]\n",
            "Epochs :  128  D_loss :  [4.8851243e-06 1.0000000e+00]  G_loss :  [301.5153, 0.29536343, 6.44724]\n",
            "Epochs :  129  D_loss :  [8.928822e-07 1.000000e+00]  G_loss :  [273.1111, 0.2669283, 6.4497666]\n",
            "Epochs :  130  D_loss :  [0.79711956 0.95      ]  G_loss :  [300.83417, 0.29527023, 5.859187]\n",
            "Epochs :  131  D_loss :  [1.1248571e-04 1.0000000e+00]  G_loss :  [330.01392, 0.31743687, 12.894476]\n",
            "Epochs :  132  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [283.94427, 0.27560526, 8.614634]\n",
            "Epochs :  133  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [267.6275, 0.26466852, 3.2236226]\n",
            "Epochs :  134  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [256.54733, 0.25196385, 4.8354406]\n",
            "Epochs :  135  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [304.40823, 0.29987237, 4.8357153]\n",
            "Epochs :  136  D_loss :  [1.3238721e-07 1.0000000e+00]  G_loss :  [249.22923, 0.24407175, 5.401543]\n",
            "Epochs :  137  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [228.85934, 0.22263475, 6.447238]\n",
            "Epochs :  138  D_loss :  [1.0960467e-07 1.0000000e+00]  G_loss :  [316.45636, 0.30712423, 9.639258]\n",
            "Epochs :  139  D_loss :  [3.9870281 0.75     ]  G_loss :  [296.9573, 0.29725456, 6.794946e-07]\n",
            "----------------------TRAINING ENDS-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0815 16:26:07.083679 140622130927488 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(480, 640, 3)\n",
            "(480, 640, 1)\n",
            "(480, 640, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD8CAYAAADzEfagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqmBoSlIFQ1OSKhiaklTh/wHiYfAQGu02PQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f415cdcf25d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m#cv2_imshow(str(i)+\"depth\", depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#depth_pred = np.squeeze(generated_img[i], axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2699\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2700\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    636\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    637\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNYZBUmMYJDWGQVJjGCQ1/wMKpFHVdp3xCwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}