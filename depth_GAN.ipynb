{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "depth-GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADI201998/sparse-to-depth-GAN/blob/master/depth_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPgZvLDqiEBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEiDNuKyMLSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY77QxKKjfIQ",
        "colab_type": "code",
        "outputId": "a3ef5ce0-d1f4-4921-9f86-85c1e404ee84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import ZeroPadding2D, Activation, BatchNormalization, MaxPooling2D, Dense, Add, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Input, Dropout\n",
        "from keras import losses\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "input_shape_generator = (480, 640, 4)\n",
        "input_shape_discriminator = (480, 640, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zyhBN_naEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_to_sparse(rgb, depth):\n",
        "        \"\"\"\n",
        "        Samples pixels with `num_samples`/#pixels probability in `depth`.\n",
        "        Only pixels with a maximum depth of `max_depth` are considered.\n",
        "        If no `max_depth` is given, samples in all pixels\n",
        "        \"\"\"\n",
        "        # max depth is np.inf\n",
        "        mask_keep = depth > 0\n",
        "        if np.inf is not np.inf:\n",
        "                mask_keep = np.bitwise_and(mask_keep, depth <= 1.0)\n",
        "        n_keep = np.count_nonzero(mask_keep)\n",
        "        if n_keep == 0:\n",
        "                return mask_keep\n",
        "        else:\n",
        "                prob = float(200) / n_keep\n",
        "                return np.bitwise_and(mask_keep, np.random.uniform(0, 1, depth.shape) < prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tas7FKuroGay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sparse_depth(rgb, depth):\n",
        "        mask_keep = dense_to_sparse(rgb, depth)\n",
        "        sparse_depth = np.zeros(depth.shape)\n",
        "        sparse_depth[mask_keep] = depth[mask_keep]\n",
        "        return sparse_depth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgeJxqfeoJM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_rgbd(rgb, depth):\n",
        "        sparse_depth = create_sparse_depth(rgb, depth)\n",
        "        rgbd = np.append(rgb, np.expand_dims(sparse_depth, axis=2), axis=2)\n",
        "        rgbd = rgbd[np.newaxis, :, :, :]\n",
        "        return rgbd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6248QBOoSJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset():\n",
        "        X_train = np.empty((1400, 480, 640, 4))\n",
        "        Y_train = np.empty((1400, 480, 640, 1))\n",
        "        X_test = np.empty((280, 480, 640, 4))\n",
        "        Y_test = np.empty((280, 480, 640, 1))\n",
        "        rootdir_train = '/content/drive/My Drive/Dataset/train/'\n",
        "        rootdir_val = '/content/drive/My Drive/Dataset/val/official/'\n",
        "        print(\"TRAIN\")\n",
        "        j = i = 0\n",
        "        for subdir, dirs, files in sorted(os.walk(rootdir_train)):\n",
        "                print(j)\n",
        "                files = sorted(files)\n",
        "                print(len(files))\n",
        "                if len(files) != 0:\n",
        "                        for k in range(5):\n",
        "                              #print(i)\n",
        "                              path = subdir + '/' + files[k]\n",
        "                              h5f = h5py.File(path, \"r\")\n",
        "                              rgb = np.array(h5f['rgb'])\n",
        "                              rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                              depth = np.array(h5f['depth'])\n",
        "                              depth  = depth /9\n",
        "                              #cv2.imshow(\"rgb\", rgb)\n",
        "                              #cv2.imshow(\"depth\", depth)\n",
        "                              X_train[i] = create_rgbd(rgb, depth)\n",
        "                              Y_train[i] = depth[:, :, np.newaxis]\n",
        "                              i = i+1\n",
        "                              #cv2.waitKey(0)\n",
        "                              #cv2.destroyAllWindows()\n",
        "                        j = j+1\n",
        "                                                \n",
        "        print(\"TEST\")\n",
        "        for subdir, dirs, files in sorted(os.walk(rootdir_val)):\n",
        "                files = sorted(files)\n",
        "                print(len(files))\n",
        "                for i in range(280):\n",
        "                        print(i)\n",
        "                        path = subdir + '/' + files[i]\n",
        "                        h5f = h5py.File(path, \"r\")\n",
        "                        rgb = np.array(h5f['rgb'])\n",
        "                        rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                        depth = np.array(h5f['depth'])\n",
        "                        depth  = depth /9\n",
        "                        X_test[i] = create_rgbd(rgb, depth)\n",
        "                        Y_test[i] = depth[:, :, np.newaxis]\n",
        "        print(\"OVER\")\n",
        "        print(\"Done\")\n",
        "        return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWA7W5hogfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model():\n",
        "    #encoder\n",
        "    X_input = Input(input_shape_generator)\n",
        "    X = Conv2D(64, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X_input)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 3,activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 1, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X1 = X\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, 1, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X2 = X\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X3 = X \n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((1,2))(X)\n",
        "\n",
        "    #decoder\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((1,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Add()([X,X3])\n",
        "    X = Dropout(0.5)(X)\n",
        "\n",
        "    X = Conv2D(512, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Add()([X,X2])\n",
        "\n",
        "    X = Conv2D(256, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Add()([X,X1])\n",
        "    X = Dropout(0.5)(X)\n",
        "\n",
        "    X = Conv2D(128, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Conv2D(64, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Dropout(0.5)(X)\n",
        "    X = Conv2D(1, 3, activation = 'relu', padding='same', kernel_initializer = 'he_normal')(UpSampling2D((2,2))(X))\n",
        "    X_out = BatchNormalization(axis=3)(X)\n",
        "\n",
        "    model = Model(input = X_input, output = X_out, name='Generator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5RD2_qIo3tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def discriminator_model():\n",
        "    X_input = Input(input_shape_discriminator)\n",
        "    X = Conv2D(64, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X_input)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(1024, kernel_size=(3,3), activation = 'relu', padding='same', kernel_initializer = 'he_normal')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Flatten()(X)\n",
        "    X_out = Dense(1, activation = 'sigmoid')(X)\n",
        "\n",
        "    model = Model(input =  X_input, output = X_out, name='Discriminator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK6HeX48s0K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_model(generator, discriminator):\n",
        "    gan_input = Input(input_shape_generator)\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    gan = Model(input = gan_input, output = [x, gan_output], name='GAN')\n",
        "    return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spUL150-tTLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 10\n",
        "epochs = 140\n",
        "input_shape = (480, 640, 4)\n",
        "# 280 x 20 \n",
        "##  INPUT  ##\n",
        "print(\"----------------------GETTING INPUTS-----------------------\")\n",
        "X_train, Y_train, X_test, Y_test = dataset()\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
        "\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "## INITIALIZE MODELS  ##\n",
        "print(\"----------------------INITIALIZING MODELS-----------------------\")\n",
        "generator = generator_model()       \n",
        "discriminator = discriminator_model()\n",
        "gan = gan_model(generator, discriminator)\n",
        "\n",
        "## COMPILE MODELS ##\n",
        "print(\"----------------------COMPILING MODELS-----------------------\")\n",
        "discriminator.trainable = True\n",
        "discriminator.compile(optimizer = Adam(4e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "discriminator.trainable = False\n",
        "gan.compile(optimizer = Adam(4e-4), loss = ['mae', 'binary_crossentropy'], loss_weights=[1, 0.001])\n",
        "discriminator.trainable = True\n",
        "\n",
        "print(\"----------------------STARTING TRAINING-----------------------\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # SELECTING RANDOM BATCH OF IMAGES                      \n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    op_imgs = Y_train[idx]\n",
        "\n",
        "    # GENERATE NEW IMAGES\n",
        "    generated_img = generator.predict(imgs)\n",
        "\n",
        "    # TRAIN DISCRIMINATOR\n",
        "    d_loss_real = discriminator.train_on_batch(op_imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_img, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    discriminator.trainable = False\n",
        "    # TRAIN GENERATOR\n",
        "    g_loss = gan.train_on_batch(imgs, [op_imgs, valid])                           #fill \n",
        "    \n",
        "    print(\"Epochs : \",epoch,\" D_loss : \", d_loss, \" G_loss : \", g_loss)\n",
        "    discriminator.trainable = True\n",
        "\n",
        "print(\"----------------------TRAINING ENDS-----------------------\")\n",
        "generator.save('generator_model.h5')\n",
        "discriminator.save('disriminator_model.h5')\n",
        "predicted_imgs = generator.predict(X_test)\n",
        "for i in range(20):\n",
        "    #rgb = np.squeeze(X_test[i], axis=0)\n",
        "    rgb = X_test[i, :, :, 0:3]\n",
        "    print(rgb.shape)\n",
        "    depth = Y_test[i, :, :, :]\n",
        "    print(depth.shape)\n",
        "    depth_pred = predicted_imgs[i, :, :, :]\n",
        "    print(depth_pred.shape)\n",
        "    plt.imshow(rgb)\n",
        "    plt.show()\n",
        "    #cv2_imshow(str(i)+\"rgb\", rgb)\n",
        "    #depth = np.squeeze(Y_test[i], axis=0)\n",
        "    \n",
        "    #cv2_imshow(str(i)+\"depth\", depth)\n",
        "    plt.imshow(depth)\n",
        "    plt.show()\n",
        "    #depth_pred = np.squeeze(generated_img[i], axis=0)\n",
        "    #cv2_imshow(str(i)+\"depth_pred\", depth_pred)\n",
        "    plt.imshow(depth_pred)\n",
        "    plt.show()\n",
        "    #cv2.waitKey(0)\n",
        "    #cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}