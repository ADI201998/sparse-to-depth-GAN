{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "depth-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADI201998/sparse-to-depth-GAN/blob/master/depth_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPgZvLDqiEBg",
        "colab_type": "code",
        "outputId": "45f11a83-c6c7-4aa6-c89c-1d7e5285f36a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MGSqjRmQN--",
        "colab_type": "code",
        "outputId": "9a4945e5-cd92-47dc-f75b-af43c3b80f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 14kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.33.6)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.1.7)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (0.16.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow",
                  "tensorflow_core"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INHneV01AMwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY77QxKKjfIQ",
        "colab_type": "code",
        "outputId": "a631d547-a30b-4e3b-8760-c139de93ef0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model, load_model, model_from_json\n",
        "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import ZeroPadding2D, Activation, BatchNormalization, MaxPooling2D, Dense, Add, Flatten, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "from keras.layers import Input, Dropout\n",
        "from keras import losses\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "input_shape_generator = (480, 640, 4)\n",
        "input_shape_discriminator = (480, 640, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zyhBN_naEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_to_sparse(rgb, depth):\n",
        "        mask_keep = depth > 0\n",
        "        if np.inf is not np.inf:\n",
        "                mask_keep = np.bitwise_and(mask_keep, depth <= np.inf)\n",
        "        n_keep = np.count_nonzero(mask_keep)\n",
        "        if n_keep == 0:\n",
        "                return mask_keep\n",
        "        else:\n",
        "                prob = float(200) / n_keep\n",
        "                return np.bitwise_and(mask_keep, np.random.uniform(0, 1, depth.shape) < prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tas7FKuroGay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            \n",
        "def create_sparse_depth(rgb, depth):\n",
        "        mask_keep = dense_to_sparse(rgb, depth)\n",
        "        sparse_depth = np.zeros(depth.shape)\n",
        "        sparse_depth[mask_keep] = depth[mask_keep]\n",
        "        return sparse_depth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgeJxqfeoJM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_rgbd(rgb, depth):\n",
        "        sparse_depth = create_sparse_depth(rgb, depth)\n",
        "        rgbd = np.append(rgb, np.expand_dims(sparse_depth, axis=2), axis=2)\n",
        "        rgbd = rgbd[np.newaxis, :, :, :]\n",
        "        return rgbd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6248QBOoSJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset():\n",
        "        #print(\"sdsdadsasd : \", n)\n",
        "        X_train = np.empty((280*6, 480, 640, 4))\n",
        "        Y_train = np.empty((280*6, 480, 640, 1))\n",
        "        rootdir_train = '/content/drive/My Drive/Dataset/train/'\n",
        "        rootdir_val = '/content/drive/My Drive/Dataset/val/official/'\n",
        "        i = 0\n",
        "        for subdir, dirs, files in sorted(os.walk(rootdir_train)):\n",
        "                files = sorted(files)\n",
        "                if len(files) != 0:\n",
        "                    for k in range(0, 6):\n",
        "                        path = subdir + '/' + files[k]\n",
        "                        h5f = h5py.File(path, \"r\")\n",
        "                        rgb = np.array(h5f['rgb'])\n",
        "                        rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                        depth = np.array(h5f['depth'])\n",
        "                        #depth  = depth\n",
        "                        X_train[i] = create_rgbd(rgb, depth)\n",
        "                        Y_train[i] = depth[:, :, np.newaxis]\n",
        "                    print(i)\n",
        "                    i = i+1\n",
        "        return X_train, Y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWA7W5hogfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model():\n",
        "    #encoder\n",
        "    X_input = Input(input_shape_generator)\n",
        "    X = Conv2D(64, 3, data_format = 'channels_last', padding='same')(X_input)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 3,data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 1, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X1 = X\n",
        "    #X = Dropout(0.2)(X)\n",
        "    X = Conv2D(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, 1, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X2 = X\n",
        "    X = Conv2D(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X) \n",
        "    X3 = X\n",
        "    X = Conv2D(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((1,2))(X)\n",
        "    #X = Dropout(0.5)(X)\n",
        "\n",
        "    #decoder\n",
        "    X = UpSampling2D((1,2))(X)\n",
        "    X = Concatenate(axis = 3)([X, X3])\n",
        "    X = Conv2DTranspose(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    \n",
        "    #X = Dropout(0.3)(X)\n",
        "    X = UpSampling2D((2,2))(X)\n",
        "    X = Concatenate(axis = 3)([X, X2])\n",
        "    X = Conv2DTranspose(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    \n",
        "    #X = Dropout(0.3)(X)\n",
        "\n",
        "    X = UpSampling2D((2,2))(X)\n",
        "    X = Concatenate(axis = 3)([X, X1])\n",
        "    X = Conv2DTranspose(256, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "   \n",
        "\n",
        "    X = Conv2DTranspose(128, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = Conv2DTranspose(64, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X = LeakyReLU()(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = Conv2DTranspose(1, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X_out = Activation('tanh')(X)\n",
        "    #X_out = BatchNormalization()(X)\n",
        "    \n",
        "\n",
        "    model = Model(input = X_input, output = X_out, name='Generator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5RD2_qIo3tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def discriminator_model():\n",
        "    X_input = Input(input_shape_discriminator)\n",
        "    X = Conv2D(64, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X_input)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dropout(0.5)(X)\n",
        "    X_out = Dense(1, activation = 'sigmoid')(X)\n",
        "\n",
        "    model = Model(input =  X_input, output = X_out, name='Discriminator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK6HeX48s0K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_model(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(input_shape_generator)\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    gan = Model(input = gan_input, output = [x, gan_output], name='GAN')\n",
        "    return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spUL150-tTLm",
        "colab_type": "code",
        "outputId": "4d94c758-adbf-4449-c034-6fa92ae8f9a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 110\n",
        "input_shape = (480, 640, 4)\n",
        "\n",
        "## INITIALIZE MODELS  ##\n",
        "print(\"----------------------INITIALIZING MODELS-----------------------\")\n",
        "generator = generator_model()       \n",
        "#discriminator = discriminator_model()\n",
        "#gan = gan_model(generator, discriminator)\n",
        "\n",
        "## COMPILE MODELS ##\n",
        "print(\"----------------------COMPILING MODELS-----------------------\")\n",
        "#discriminator.compile(optimizer = Adam(1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "#gan.compile(optimizer = Adam(1e-4), loss = ['mae', 'binary_crossentropy'], loss_weights=[0.999, 0.001])\n",
        "generator.compile(optimizer = Adam(1e-3), loss = 'mae')\n",
        "i = 0\n",
        "\n",
        "##  INPUT  ##\n",
        "print(\"----------------------GETTING INPUTS-----------------------\")\n",
        "X_train, Y_train = dataset()\n",
        "X_train[:,:,:,0:3] = (X_train[:,:,:,0:3] - (X_train[:,:,:,0:3].max()/2))/X_train[:,:,:,0:3].max()\n",
        "X_train[:,:,:,3] = (X_train[:,:,:,3] - (X_train[:,:,:,3].max()/2))/X_train[:,:,:,3].max()\n",
        "#print(\"Y max = \", Y_train.max(), \" Y min = \", Y_train.min(), \" Y mean = \", Y_train.mean())\n",
        "Y_train = (Y_train - (Y_train.max()/2))/Y_train.max()\n",
        "#print(\"Y max = \", Y_train.max(), \" Y min = \", Y_train.min(), \" Y mean = \", Y_train.mean())\n",
        "#print(X_train.shape, Y_train.shape)\n",
        "\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "#gen = load_model('/content/generator_model.h5')\n",
        "\n",
        "print(\"----------------------STARTING TRAINING-----------------------\")\n",
        "\n",
        "#print(discriminator.metrics_names, gan.metrics_names)\n",
        "print(generator.metrics_names)\n",
        "'''''''''\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # SELECTING RANDOM BATCH OF IMAGES                      \n",
        "  idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "  imgs = X_train[idx]\n",
        "  op_imgs = Y_train[idx]\n",
        "\n",
        "  # GENERATE NEW IMAGES\n",
        "  generated_img = generator.predict(imgs)\n",
        "\n",
        "  discriminator.trainable = True\n",
        "\n",
        "  # TRAIN DISCRIMINATOR\n",
        "  d_loss_real = discriminator.train_on_batch(op_imgs, valid)\n",
        "  d_loss_fake = discriminator.train_on_batch(generated_img, fake)\n",
        "  d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "  discriminator.trainable = False\n",
        "  idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "  imgs = X_train[idx]\n",
        "  op_imgs = Y_train[idx]\n",
        "  # TRAIN GENERATOR\n",
        "  g_loss = gan.train_on_batch(imgs, [op_imgs, valid])                           \n",
        "\n",
        "  print(\"Epochs : \",epoch,\" D_loss : \", d_loss, \" G_loss : \", g_loss)\n",
        "  \n",
        "\n",
        "print(\"----------------------TRAINING ENDS-----------------------\")\n",
        "generator.save('generator_model.h5')\n",
        "discriminator.save('disriminator_model.h5')\n",
        "'''''''''\n",
        "generator.fit(x = X_train, y = Y_train, batch_size = 10, epochs = 20, shuffle=True, validation_split=0.2)\n",
        "generator.save('generator_model.h5')\n",
        "json_string = generator.to_json()\n",
        "open('generator.json', 'w').write(json_string)\n",
        "generator.save_weights('weights.h5')\n",
        "print(\"DOWNLOADING.....\")\n",
        "model_file1 = drive.CreateFile({'title' : 'generator_model.h5'})\n",
        "model_file2 = drive.CreateFile({'title' : 'generator.json'})\n",
        "model_file3 = drive.CreateFile({'title' : 'weights.h5'})\n",
        "model_file1.SetContentFile('generator_model.h5')\n",
        "model_file2.SetContentFile('weights.h5')\n",
        "model_file3.SetContentFile('generator.json')\n",
        "model_file1.Upload()\n",
        "model_file2.Upload()\n",
        "model_file3.Upload()\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file1.get('id')})\n",
        "drive.CreateFile({'id': model_file2.get('id')})\n",
        "drive.CreateFile({'id': model_file3.get('id')})\n",
        "generator = load_model('generator_model.h5')\n",
        "#files.download('generator.json')\n",
        "#files.download('weights.h5')\n",
        "print(\"DONE.....\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------INITIALIZING MODELS-----------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "----------------------COMPILING MODELS-----------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Generator\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------GETTING INPUTS-----------------------\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "----------------------STARTING TRAINING-----------------------\n",
            "['loss']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 1344 samples, validate on 336 samples\n",
            "Epoch 1/20\n",
            "1344/1344 [==============================] - 1319s 981ms/step - loss: 0.2386 - val_loss: 0.1964\n",
            "Epoch 2/20\n",
            "1344/1344 [==============================] - 1417s 1s/step - loss: 0.0802 - val_loss: 0.1181\n",
            "Epoch 3/20\n",
            "1344/1344 [==============================] - 1409s 1s/step - loss: 0.0573 - val_loss: 0.2300\n",
            "Epoch 4/20\n",
            "1344/1344 [==============================] - 1300s 967ms/step - loss: 0.0584 - val_loss: 0.2182\n",
            "Epoch 5/20\n",
            "1344/1344 [==============================] - 1275s 948ms/step - loss: 0.0538 - val_loss: 0.1198\n",
            "Epoch 6/20\n",
            "1344/1344 [==============================] - 1341s 997ms/step - loss: 0.0513 - val_loss: 0.2472\n",
            "Epoch 7/20\n",
            "1344/1344 [==============================] - 1409s 1s/step - loss: 0.0488 - val_loss: 0.2368\n",
            "Epoch 8/20\n",
            "1344/1344 [==============================] - 1543s 1s/step - loss: 0.0503 - val_loss: 0.0658\n",
            "Epoch 9/20\n",
            "1344/1344 [==============================] - 1473s 1s/step - loss: 0.0476 - val_loss: 0.4975\n",
            "Epoch 10/20\n",
            "1344/1344 [==============================] - 1466s 1s/step - loss: 0.0508 - val_loss: 1.0295\n",
            "Epoch 11/20\n",
            "1344/1344 [==============================] - 1345s 1s/step - loss: 0.0484 - val_loss: 0.2660\n",
            "Epoch 12/20\n",
            "1344/1344 [==============================] - 1365s 1s/step - loss: 0.0444 - val_loss: 0.2347\n",
            "Epoch 13/20\n",
            "1344/1344 [==============================] - 1352s 1s/step - loss: 0.0432 - val_loss: 0.1915\n",
            "Epoch 14/20\n",
            "1344/1344 [==============================] - 1388s 1s/step - loss: 0.0431 - val_loss: 0.3310\n",
            "Epoch 15/20\n",
            "1344/1344 [==============================] - 1375s 1s/step - loss: 0.0430 - val_loss: 0.3228\n",
            "Epoch 16/20\n",
            "1344/1344 [==============================] - 1384s 1s/step - loss: 0.0413 - val_loss: 0.3972\n",
            "Epoch 17/20\n",
            "1344/1344 [==============================] - 1355s 1s/step - loss: 0.0411 - val_loss: 0.2095\n",
            "Epoch 18/20\n",
            "1344/1344 [==============================] - 1472s 1s/step - loss: 0.0534 - val_loss: 0.1790\n",
            "Epoch 19/20\n",
            "1344/1344 [==============================] - 1440s 1s/step - loss: 0.0397 - val_loss: 0.0272\n",
            "Epoch 20/20\n",
            "1344/1344 [==============================] - 1466s 1s/step - loss: 0.0367 - val_loss: 0.2224\n",
            "DOWNLOADING.....\n",
            "DONE.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWKcF9uijl-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = np.empty((168, 480, 640, 4))\n",
        "Y_test = np.empty((168, 480, 640, 1))\n",
        "rootdir_train = '/content/drive/My Drive/Dataset/train/'\n",
        "rootdir_val = '/content/drive/My Drive/Dataset/val/official/'\n",
        "i = 0\n",
        "for subdir, dirs, files in sorted(os.walk(rootdir_val)):\n",
        "        files = sorted(files)\n",
        "        for k in range(168):\n",
        "            path = subdir + '/' + files[k]\n",
        "            h5f = h5py.File(path, \"r\")\n",
        "            rgb = np.array(h5f['rgb'])\n",
        "            rgb = np.transpose(rgb, (1, 2, 0))\n",
        "            depth = np.array(h5f['depth'])\n",
        "            depth  = depth\n",
        "            X_test[i] = create_rgbd(rgb, depth)\n",
        "            Y_test[i] = depth[:, :, np.newaxis]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7K9cJUGa8aO",
        "colab_type": "code",
        "outputId": "89daa4ea-69a4-4417-c4ae-0ba67505e1b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ls drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1558265200149343868479.jpg\n",
            "'20180712_213250[1].jpg'\n",
            " 2.txt.gdoc\n",
            " 881031133020286_signed.pdf\n",
            "'Assignment 4.doc'\n",
            "'AttachmentPhoto .jpg'\n",
            "'B S Grewal(Password - eduinformer) (1).pdf'\n",
            "'B S Grewal(Password - eduinformer).pdf'\n",
            "'class (1).cpp'\n",
            "'class (2).cpp'\n",
            " class.cpp\n",
            " Classroom\n",
            "'Colab Notebooks'\n",
            "'CV (1).docx'\n",
            " CV.docx\n",
            " Dataset\n",
            " Docs\n",
            "'E & AI Report01_28_07_2018 (copy) (1).docx.gdoc'\n",
            "'E & AI Report01_28_07_2018 (copy).docx.gdoc'\n",
            "'E & AI Report01_28_07_2018.gdoc'\n",
            "'E & AI Report_06_15_09_2018.docx'\n",
            "'E & AI Report_06_15_09_2018.gdoc'\n",
            " education_Column.pdf\n",
            "'generator (1).json'\n",
            " generator.json\n",
            "'generator_model (1).h5'\n",
            " generator_model.h5\n",
            " heatransfer_ch02.pdf.gdoc\n",
            "'hoodies-changed (1).gdoc'\n",
            "'How to get started with Drive.pdf'\n",
            "'III MTRON'\n",
            " IMG-20180103-WA0005.jpg\n",
            "'Industrial Training Report format 2018.gdoc'\n",
            "'Installing OpenCV on Linux.gdoc'\n",
            " INternship_Certificate.pdf\n",
            "'L2 60.pdf'\n",
            "'L7 Iteration.pdf'\n",
            "'L8 Lists.pdf'\n",
            "'L9 Functions.pdf'\n",
            "'Letter of Validation (1).pdf'\n",
            "'Letter of Validation.pdf'\n",
            "'MAdityaSharma_160929084_MT (5) (1).pdf'\n",
            "'MAdityaSharma_160929084_MT (5) - Aditya Sharma.pdf'\n",
            "'MAdityaSharma_160929084_MT (5).pdf'\n",
            "'M Aditya Sharma Resume (1).pdf'\n",
            "'M Aditya Sharma Resume (2).pdf'\n",
            "'M Aditya Sharma Resume (3).pdf'\n",
            "'M Aditya Sharma Resume (4).pdf'\n",
            "'M Aditya Sharma Resume - Aditya Sharma.pdf'\n",
            "'M Aditya Sharma Resume.gdoc'\n",
            "'M Aditya Sharma Resume.pdf'\n",
            "'Mechatronics 3rd sem'\n",
            "'Mechatronics 4th sem'\n",
            "'New Doc 2019-05-13 13.57.58.pdf'\n",
            "'PARTS SOURCING.gsheet'\n",
            " Photos\n",
            "'pinout for black panther (doc format).gdoc'\n",
            " Python_basic.pdf\n",
            "'SEC A'\n",
            " SOM\n",
            "'The Quick Python Book, Second Edition (2010).pdf'\n",
            "'University Rover Challenge Rules 2018.pdf'\n",
            "'weights (1).h5'\n",
            " weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZX7CGVczEQs",
        "colab_type": "code",
        "outputId": "65f94390-422d-4690-8784-d80bebd6253c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "valid = np.ones((8, 1))\n",
        "fake = np.zeros((168, 1))\n",
        "k = 0\n",
        "for i in range(1,22):\n",
        "        imgs = generator.predict(X_test[k:8*i, :, :, :])\n",
        "        l1 = generator.evaluate(x = (X_test[k:8*i, :, :, :]-127.5)/127.5, y = (Y_test[k:8*i, :, :, :]-Y_train[k:8*i, :, :, :].mean())/Y_train[k:8*i, :, :, :].max())\n",
        "        print(\"L1 : \",l1)\n",
        "        k = k+8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r8/8 [==============================] - 3s 385ms/step\n",
            "L1 :  2.957366943359375\n",
            "8/8 [==============================] - 2s 302ms/step\n",
            "L1 :  1.803611397743225\n",
            "8/8 [==============================] - 2s 306ms/step\n",
            "L1 :  2.0274205207824707\n",
            "8/8 [==============================] - 2s 305ms/step\n",
            "L1 :  2.9999921321868896\n",
            "8/8 [==============================] - 2s 302ms/step\n",
            "L1 :  2.0498318672180176\n",
            "8/8 [==============================] - 2s 305ms/step\n",
            "L1 :  5.870504379272461\n",
            "8/8 [==============================] - 2s 300ms/step\n",
            "L1 :  1.6967644691467285\n",
            "8/8 [==============================] - 2s 303ms/step\n",
            "L1 :  3.1392881870269775\n",
            "8/8 [==============================] - 2s 301ms/step\n",
            "L1 :  1.6259678602218628\n",
            "8/8 [==============================] - 2s 301ms/step\n",
            "L1 :  2.623948097229004\n",
            "8/8 [==============================] - 2s 303ms/step\n",
            "L1 :  1.7515509128570557\n",
            "8/8 [==============================] - 2s 300ms/step\n",
            "L1 :  7.243819236755371\n",
            "8/8 [==============================] - 2s 304ms/step\n",
            "L1 :  1.8667833805084229\n",
            "8/8 [==============================] - 2s 300ms/step\n",
            "L1 :  5.462560653686523\n",
            "8/8 [==============================] - 2s 301ms/step\n",
            "L1 :  1.330765724182129\n",
            "8/8 [==============================] - 2s 304ms/step\n",
            "L1 :  1.8163561820983887\n",
            "8/8 [==============================] - 3s 314ms/step\n",
            "L1 :  1.2803740501403809\n",
            "8/8 [==============================] - 2s 305ms/step\n",
            "L1 :  1.3605749607086182\n",
            "8/8 [==============================] - 2s 304ms/step\n",
            "L1 :  1.4801275730133057\n",
            "8/8 [==============================] - 2s 304ms/step\n",
            "L1 :  1.2782959938049316\n",
            "8/8 [==============================] - 2s 301ms/step\n",
            "L1 :  1.411028504371643\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}