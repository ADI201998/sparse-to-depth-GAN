{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "depth-GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADI201998/sparse-to-depth-GAN/blob/master/depth_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPgZvLDqiEBg",
        "colab_type": "code",
        "outputId": "81c6311b-a85e-4145-9f14-f2f2af8861a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MGSqjRmQN--",
        "colab_type": "code",
        "outputId": "ec2c3ee2-6a4c-415c-9143-9c1d78b81763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 80kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INHneV01AMwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY77QxKKjfIQ",
        "colab_type": "code",
        "outputId": "ed468c98-8b0c-43b2-cbb8-dbfa6dc3fcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import ZeroPadding2D, Activation, BatchNormalization, MaxPooling2D, Dense, Add, Flatten, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Input, Dropout\n",
        "from keras import losses\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "input_shape_generator = (480, 640, 4)\n",
        "input_shape_discriminator = (480, 640, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zyhBN_naEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_to_sparse(rgb, depth):\n",
        "        mask_keep = depth > 0\n",
        "        if np.inf is not np.inf:\n",
        "                mask_keep = np.bitwise_and(mask_keep, depth <= np.inf)\n",
        "        n_keep = np.count_nonzero(mask_keep)\n",
        "        if n_keep == 0:\n",
        "                return mask_keep\n",
        "        else:\n",
        "                prob = float(200) / n_keep\n",
        "                return np.bitwise_and(mask_keep, np.random.uniform(0, 1, depth.shape) < prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tas7FKuroGay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            \n",
        "def create_sparse_depth(rgb, depth):\n",
        "        mask_keep = dense_to_sparse(rgb, depth)\n",
        "        sparse_depth = np.zeros(depth.shape)\n",
        "        sparse_depth[mask_keep] = depth[mask_keep]\n",
        "        return sparse_depth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgeJxqfeoJM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_rgbd(rgb, depth):\n",
        "        sparse_depth = create_sparse_depth(rgb, depth)\n",
        "        rgbd = np.append(rgb, np.expand_dims(sparse_depth, axis=2), axis=2)\n",
        "        rgbd = rgbd[np.newaxis, :, :, :]\n",
        "        return rgbd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6248QBOoSJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset():\n",
        "        X_train = np.empty((280*6, 480, 640, 4))\n",
        "        Y_train = np.empty((280*6, 480, 640, 1))\n",
        "        rootdir_train = '/content/drive/My Drive/Dataset/train/'\n",
        "        rootdir_val = '/content/drive/My Drive/Dataset/val/official/'\n",
        "        i = 0\n",
        "        for subdir, dirs, files in sorted(os.walk(rootdir_train)):\n",
        "                files = sorted(files)\n",
        "                if len(files) != 0:\n",
        "                    for k in range(6):\n",
        "                        path = subdir + '/' + files[k]\n",
        "                        h5f = h5py.File(path, \"r\")\n",
        "                        rgb = np.array(h5f['rgb'])\n",
        "                        rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                        depth = np.array(h5f['depth'])\n",
        "                        depth  = depth /9\n",
        "                        X_train[i] = create_rgbd(rgb, depth)\n",
        "                        Y_train[i] = depth[:, :, np.newaxis]\n",
        "                    print(i)\n",
        "                    i = i+1\n",
        "        return X_train, Y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWA7W5hogfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model():\n",
        "    #encoder\n",
        "    X_input = Input(input_shape_generator)\n",
        "    X = Conv2D(64, 3, data_format = 'channels_last', padding='same')(X_input)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 3,data_format = 'channels_last', padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, 1, data_format = 'channels_last', padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X1 = X\n",
        "    X = Conv2D(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, 1, data_format = 'channels_last', padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X2 = X\n",
        "    X = Conv2D(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((2,2))(X) \n",
        "    X3 = X\n",
        "    X = Conv2D(512, 3, data_format = 'channels_last', padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((1,2))(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Dropout(0.4)(X)\n",
        "\n",
        "    #decoder\n",
        "    X = Conv2DTranspose(512, 3, data_format = 'channels_last', padding='same')(UpSampling2D((1,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Concatenate(axis = 3)([X, X3])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    #X = Dropout(0.3)(X)\n",
        "\n",
        "    X = Conv2DTranspose(512, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Concatenate(axis = 3)([X, X2])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    #X = Dropout(0.3)(X)\n",
        "\n",
        "    X = Conv2DTranspose(256, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Concatenate(axis = 3)([X, X1])\n",
        "    X = Activation('relu')(X)\n",
        "   \n",
        "\n",
        "    X = Conv2DTranspose(128, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2DTranspose(64, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2DTranspose(1, 3, data_format = 'channels_last', padding='same')(UpSampling2D((2,2))(X))\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X_out = Activation('tanh')(X)\n",
        "\n",
        "    model = Model(input = X_input, output = X_out, name='Generator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5RD2_qIo3tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def discriminator_model():\n",
        "    X_input = Input(input_shape_discriminator)\n",
        "    X = Conv2D(64, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X_input)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(128, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(256, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Conv2D(512, kernel_size=(3,3), data_format = 'channels_last', padding='same')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Dropout(0.3)(X)\n",
        "    X = MaxPooling2D((2,2))(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dropout(0.5)(X)\n",
        "    X_out = Dense(1, activation = 'sigmoid')(X)\n",
        "\n",
        "    model = Model(input =  X_input, output = X_out, name='Discriminator')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK6HeX48s0K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_model(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(input_shape_generator)\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    gan = Model(input = gan_input, output = [x, gan_output], name='GAN')\n",
        "    return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spUL150-tTLm",
        "colab_type": "code",
        "outputId": "204ad656-8616-490a-b88b-993b5671ad00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 110\n",
        "input_shape = (480, 640, 4)\n",
        "\n",
        "## INITIALIZE MODELS  ##\n",
        "print(\"----------------------INITIALIZING MODELS-----------------------\")\n",
        "generator = generator_model()       \n",
        "#discriminator = discriminator_model()\n",
        "#gan = gan_model(generator, discriminator)\n",
        "\n",
        "## COMPILE MODELS ##\n",
        "print(\"----------------------COMPILING MODELS-----------------------\")\n",
        "#discriminator.compile(optimizer = Adam(1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "#gan.compile(optimizer = Adam(1e-4), loss = ['mae', 'binary_crossentropy'], loss_weights=[0.999, 0.001])\n",
        "generator.compile(optimizer = Adam(8e-3), loss = 'mae')\n",
        "i = 0\n",
        "\n",
        "##  INPUT  ##\n",
        "print(\"----------------------GETTING INPUTS-----------------------\")\n",
        "X_train, Y_train = dataset()\n",
        "X_train = (X_train - 127.5)/127.5\n",
        "Y_train = (Y_train - 127.5)/127.5\n",
        "#print(X_train.shape, Y_train.shape)\n",
        "\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "#gen = load_model('/content/generator_model.h5')\n",
        "\n",
        "print(\"----------------------STARTING TRAINING-----------------------\")\n",
        "\n",
        "#print(discriminator.metrics_names, gan.metrics_names)\n",
        "print(generator.metrics_names)\n",
        "'''''''''\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # SELECTING RANDOM BATCH OF IMAGES                      \n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    op_imgs = Y_train[idx]\n",
        "\n",
        "    # GENERATE NEW IMAGES\n",
        "    generated_img = generator.predict(imgs)\n",
        "\n",
        "    discriminator.trainable = True\n",
        "\n",
        "    # TRAIN DISCRIMINATOR\n",
        "    d_loss_real = discriminator.train_on_batch(op_imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_img, fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    discriminator.trainable = False\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    op_imgs = Y_train[idx]\n",
        "    # TRAIN GENERATOR\n",
        "    g_loss = gan.train_on_batch(imgs, [op_imgs, valid])                           \n",
        "\n",
        "    print(\"Epochs : \",epoch,\" D_loss : \", d_loss, \" G_loss : \", g_loss)\n",
        "    \n",
        "\n",
        "print(\"----------------------TRAINING ENDS-----------------------\")\n",
        "generator.save('generator_model.h5')\n",
        "discriminator.save('disriminator_model.h5')\n",
        "'''''''''\n",
        "generator.fit(x = X_train, y = Y_train, batch_size = 8, epochs = 10, shuffle=True, validation_split=0.2)\n",
        "generator.save('generator_model.h5')\n",
        "json_string = generator.to_json()\n",
        "open('generator.json', 'w').write(json_string)\n",
        "generator.save_weights('weights.h5')\n",
        "print(\"DOWNLOADING.....\")\n",
        "model_file1 = drive.CreateFile({'title' : 'generator_model.h5'})\n",
        "model_file2 = drive.CreateFile({'title' : 'generator.json'})\n",
        "model_file3 = drive.CreateFile({'title' : 'weights.h5'})\n",
        "model_file1.SetContentFile('generator_model.h5')\n",
        "model_file2.SetContentFile('weights.h5')\n",
        "model_file3.SetContentFile('generator.json')\n",
        "model_file1.Upload()\n",
        "model_file2.Upload()\n",
        "model_file3.Upload()\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file1.get('id')})\n",
        "drive.CreateFile({'id': model_file2.get('id')})\n",
        "drive.CreateFile({'id': model_file3.get('id')})\n",
        "#files.download('generator.json')\n",
        "#files.download('weights.h5')\n",
        "print(\"DONE.....\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------INITIALIZING MODELS-----------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "----------------------COMPILING MODELS-----------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Generator\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------GETTING INPUTS-----------------------\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "----------------------STARTING TRAINING-----------------------\n",
            "['loss']\n",
            "Train on 1344 samples, validate on 336 samples\n",
            "Epoch 1/20\n",
            "1344/1344 [==============================] - 287s 213ms/step - loss: 0.4138 - val_loss: 0.1002\n",
            "Epoch 2/20\n",
            "1344/1344 [==============================] - 272s 203ms/step - loss: 0.1154 - val_loss: 0.0544\n",
            "Epoch 3/20\n",
            "1344/1344 [==============================] - 272s 202ms/step - loss: 0.0510 - val_loss: 0.0263\n",
            "Epoch 4/20\n",
            "1344/1344 [==============================] - 272s 203ms/step - loss: 0.0282 - val_loss: 0.0173\n",
            "Epoch 5/20\n",
            "1344/1344 [==============================] - 272s 203ms/step - loss: 0.0181 - val_loss: 0.0179\n",
            "Epoch 6/20\n",
            "1344/1344 [==============================] - 272s 203ms/step - loss: 0.0126 - val_loss: 0.0073\n",
            "Epoch 7/20\n",
            "1344/1344 [==============================] - 272s 203ms/step - loss: 0.0104 - val_loss: 0.0559\n",
            "Epoch 8/20\n",
            "1344/1344 [==============================] - 272s 202ms/step - loss: 0.0079 - val_loss: 0.0055\n",
            "Epoch 9/20\n",
            "1344/1344 [==============================] - 272s 202ms/step - loss: 0.0078 - val_loss: 0.0055\n",
            "Epoch 10/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0058 - val_loss: 0.0035\n",
            "Epoch 11/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0051 - val_loss: 0.0028\n",
            "Epoch 12/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0048 - val_loss: 0.0035\n",
            "Epoch 13/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0047 - val_loss: 0.0025\n",
            "Epoch 14/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0043 - val_loss: 0.0021\n",
            "Epoch 15/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0040 - val_loss: 0.0018\n",
            "Epoch 16/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 17/20\n",
            "1344/1344 [==============================] - 272s 202ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 18/20\n",
            "1344/1344 [==============================] - 272s 202ms/step - loss: 0.0035 - val_loss: 0.0012\n",
            "Epoch 19/20\n",
            "1344/1344 [==============================] - 271s 202ms/step - loss: 0.0033 - val_loss: 9.9075e-04\n",
            "Epoch 20/20\n",
            "1344/1344 [==============================] - 272s 202ms/step - loss: 0.0032 - val_loss: 8.1556e-04\n",
            "DOWNLOADING.....\n",
            "DONE.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWKcF9uijl-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = np.empty((168, 480, 640, 4))\n",
        "Y_test = np.empty((168, 480, 640, 1))\n",
        "rootdir_train = '/content/drive/My Drive/Dataset/train/'\n",
        "rootdir_val = '/content/drive/My Drive/Dataset/val/official/'\n",
        "i = 0\n",
        "for subdir, dirs, files in sorted(os.walk(rootdir_val)):\n",
        "        files = sorted(files)\n",
        "        for k in range(168):\n",
        "            path = subdir + '/' + files[k]\n",
        "            h5f = h5py.File(path, \"r\")\n",
        "            rgb = np.array(h5f['rgb'])\n",
        "            rgb = np.transpose(rgb, (1, 2, 0))\n",
        "            depth = np.array(h5f['depth'])\n",
        "            depth  = depth /9\n",
        "            X_test[i] = create_rgbd(rgb, depth)\n",
        "            Y_test[i] = depth[:, :, np.newaxis]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZX7CGVczEQs",
        "colab_type": "code",
        "outputId": "bdee1476-438b-4e40-cc8c-1f5b53e6c304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "valid = np.ones((8, 1))\n",
        "fake = np.zeros((168, 1))\n",
        "k = 0\n",
        "for i in range(1,22):\n",
        "        imgs = generator.predict(X_test[k:8*i, :, :, :])\n",
        "        l1 = generator.evaluate(x = (X_test[k:8*i, :, :, :]-127.5)/127.5, y = (Y_test[k:8*i, :, :, :]-127.5)/127.5)\n",
        "        print(\"L1 : \",l1)\n",
        "        k = k+8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r8/8 [==============================] - 1s 89ms/step\n",
            "L1 :  0.0016242156270891428\n",
            "8/8 [==============================] - 1s 86ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.0008155559189617634\n",
            "8/8 [==============================] - 1s 71ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 71ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 73ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 73ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 71ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 73ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 73ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 71ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 72ms/step\n",
            "L1 :  0.000815560226328671\n",
            "8/8 [==============================] - 1s 73ms/step\n",
            "L1 :  0.000815560226328671\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}